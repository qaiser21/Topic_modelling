{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Package Used\n",
    "    \n",
    "    --nltk Package has been used for data preprocessing\n",
    "       . stopword removal\n",
    "       . special character removed\n",
    "       . part of speech tag(pos)\n",
    "       . tokenizing\n",
    "       . lemmatizing noun from tokenize token\n",
    "       \n",
    "    --CountVectorizer from sklearn to convert text into its Vector Form \n",
    "    --LDA(LatentDirichletAllocation) is used from sklearn, LDA can also be used using Gensim module\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Data PreProcessing Step\n",
    "   \n",
    "   Class Name NLTKPreprocessor is made using BaseEstimator transformerMixin for data \n",
    "   preprocessing step\n",
    "   \n",
    "   identity function is created which is used in tokenize argument of CountVectorizer \n",
    "   which bypasses the tokenize function\n",
    "   \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True):\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(document):\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If stopword, ignore token and continue\n",
    "                if token in self.stopwords:\n",
    "                    continue\n",
    "\n",
    "                # If punctuation, ignore token and continue\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                yield lemma\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessor = NLTKPreprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#  Data Used\n",
    "    \n",
    "    link - http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "    ACL paper - 2011, \n",
    "    Author = = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and    \n",
    "                Potts, Christopher},\n",
    "    title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "    \n",
    "    \n",
    "    only test data is used for Topic Modelling which is of size 25000\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_files('/aclImdb/test')\n",
    "train_data = data.data\n",
    "text_train=[x.decode('utf-8') for x in train_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Preprocessing Extracted Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocess_data=preprocessor.transform(text_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorizing word using CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features=10000, max_df=.15,tokenizer=identity, preprocessor=None, lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=0.15, max_features=10000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function identity at 0x7f15a8225268>, vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorize_data=vect.fit_transform(preprocess_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA package\n",
    "  \n",
    "  Number of topic for which whole  document is divided is 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics = 10, learning_method=\"batch\",max_iter=25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=25, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=1, n_topics=10, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/techm/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "document_topics = lda.fit_transform(vectorize_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Topic Visualisation\n",
    "   \n",
    "     used mglearn package for topic visualisation\n",
    "    \n",
    "     \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_topics(topics, feature_names, sorting, topics_per_chunk=6,n_words=20):\n",
    "    for i in range(0,len(topics),topics_per_chunk):\n",
    "        these_topics = topics[i : i + topics_per_chunk]\n",
    "        len_this_chunk = len(these_topics)\n",
    "        print((\"topic {:<8}\" * len_this_chunk).format(*these_topics))\n",
    "        print((\"-------- {0:<5}\" * len_this_chunk).format(\"\"))\n",
    "        for i in range(n_words):\n",
    "            try:\n",
    "                print((\"{:<14}\" * len_this_chunk).format(*feature_names[sorting[these_topics, i]]))\n",
    "            except:\n",
    "                pass\n",
    "        print(\"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "book          role          funny         guy           kill          \n",
      "read          performance   comedy        minute        black         \n",
      "dvd           song          kid           actually      killer        \n",
      "series        star          laugh         horror        woman         \n",
      "tv            cast          joke          nothing       action        \n",
      "novel         music         fun           2             Â–             \n",
      "version       john          old           10            drug          \n",
      "saw           musical       game          stupid        murder        \n",
      "original      big           guy           act           cop           \n",
      "write         comedy        episode       waste         death         \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "music         script        war           effect        woman         \n",
      "feel          action        world         horror        girl          \n",
      "real          original      u             episode       father        \n",
      "beautiful     cast          american      monster       young         \n",
      "u             lack          point         series        wife          \n",
      "audience      performance   woman         alien         mother        \n",
      "cinema        nothing       become        space         family        \n",
      "long          act           soldier       special       daughter      \n",
      "leave         poor          country       vampire       house         \n",
      "performance   enough        death         fi            old           \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorting = np.argsort(lda.components_,axis=1)[:,::-1]\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "print_topics(topics = range(10), feature_names = feature_names, sorting=sorting, topics_per_chunk=5,n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  Pipeline([('preprocess',NLTKPreprocessor()),\\\n",
    "                      ('vect',CountVectorizer(max_features=10000, max_df=.15,tokenizer=identity, preprocessor=None, lowercase=False)),\\\n",
    "                       ('lda', LatentDirichletAllocation(n_topics = 10, learning_method=\"batch\",max_iter=25, random_state=0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preprocess', NLTKPreprocessor(lower=True,\n",
       "         punct={'.', '|', '?', '#', '*', '\"', \"'\", ';', '&', ')', '(', '/', '`', ']', '>', '<', '=', '-', '@', '[', '^', '!', '}', '~', '_', '%', '$', '+', ',', '{', '\\\\', ':'},\n",
       "         stopwords={'so', \"isn't\", 'after', 'o', 'our', \"she's\", 'itsel...           random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/techm/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preprocess', NLTKPreprocessor(lower=True,\n",
       "         punct={'.', '|', '?', '#', '*', '\"', \"'\", ';', '&', ')', '(', '/', '`', ']', '>', '<', '=', '-', '@', '[', '^', '!', '}', '~', '_', '%', '$', '+', ',', '{', '\\\\', ':'},\n",
       "         stopwords={'so', \"isn't\", 'after', 'o', 'our', \"she's\", 'itsel...           random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00333389,  0.00333394,  0.00333413,  0.00333412,  0.00333376,\n",
       "         0.00333413,  0.00333407,  0.96999388,  0.00333415,  0.00333393]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(['Hacksaw Ridge is a movie that shows us the pain of war.\\\n",
    "                  It brings our mind to virtualize a whole lot of things\\\n",
    "                  about the life of soldiers and their struggle to save the nation.\\\n",
    "                  It shows the real picture of world war-II and the real life history\\\n",
    "                  of an army man Doss. The movie is really a tribute to Doss and also\\\n",
    "                  to each and every army soldiers. Though the movie is not humorous, it\\\n",
    "                  shows the humanity towards mankind.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSION\n",
    "\n",
    "By Seeing the word of topic we can conclude about the movie review content , like topic 5 deal with music related, topic 7 is related with war related, topic made with Horror related and so on\n",
    "\n",
    "For predicting the topic of new movie review we just need to create a pipeline of all three preprocess and use model transform function. Topic having maximum probability will belong to that topic\n",
    "\n",
    "We can see that the comment for above Hacksaw ridge movie, the score with 7 position(indexing starting from 0) is maximum which belong to topic 7, which clearly indicate movie genre which is war\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
